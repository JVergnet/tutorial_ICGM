{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLINE\n",
    " 1 Python basics : Setup, syntax & standard libraries\n",
    "        \n",
    "2 Pymatgen 1 : build a structure & write a vasp job\n",
    "    \n",
    "3 Python + slurm + bash : launch a vasp job\n",
    "    \n",
    "4 Pymatgen 2 : read a vasp job & plot DOS\n",
    "    \n",
    "5 Matplotlib : make your own plots\n",
    "    \n",
    "6 Pymatgen 3 : advanced functionalities (disorder, bader, lobster ... )\n",
    "    \n",
    "__7 the read_write library : installation, work flow & examples__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-4694300982b8>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-4694300982b8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    __WRITE LOOP__\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# read_write  : vasp workflow at the ICGM\n",
    "## vasp workflow : \n",
    "__WRITE LOOP__\n",
    "* get cif from internet / experimentalists \n",
    "* modify cif \n",
    "* generate vasp input (POTCAR, INCAR) \n",
    "* `launch jobs`\n",
    "\n",
    "__RERUN LOOP__\n",
    "* read existing job (converged or not)\n",
    "* modify incar (create \"rerun\" vasp input file)\n",
    "* `launch \"rerun\" jobs`\n",
    "\n",
    "__READ LOOP__\n",
    "* anayze converged job \n",
    "* write \"analysis\" vasp job (phonopy, lobster, band structure, ... )\n",
    "* `launch \"analysis\" jobs`\n",
    "* __RERUN LOOP__ for \"analysis\" jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (<ipython-input-15-a681bd937174>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a681bd937174>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    * WRITE\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "## 4 ACTIONS : \n",
    "* WRITE \n",
    "* LAUNCH\n",
    "* RERUN \n",
    "* READ"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-b09e2bb6f215>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-b09e2bb6f215>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    WRITE LOOP => python3 ./write_job.py\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "WRITE LOOP => python3 ./write_job.py\n",
    "\n",
    "* get cif from internet / experimentalists\n",
    "* modify cif (ordering, desodiation, distortion, layer rotation, slabs, => __ADD YOUR OWN__  ! \n",
    "* generate vasp input (POTCAR, INCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-3f0b24a5857b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3f0b24a5857b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    LAUNCH => python3 ./launch_job.py\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "LAUNCH => python3 ./launch_job.py\n",
    "\n",
    "ask for sbatch parameters (name, nb core / job, QOS, ...)\n",
    "\n",
    "ask for job executable parameters (single, double,...)\n",
    "\n",
    "ask array or individual launch \n",
    "* if array : launch 1 job / subfolder all at once \n",
    "* else : ask for each subfolder if user want to launch is as a job"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-d59077eb543f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-d59077eb543f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    RERUN => python3 ./rerun_job.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "RERUN => python3 ./rerun_job.py\n",
    "\n",
    "ask for rerun type (single point, relaxation, custom ...) \n",
    "\n",
    "ask for more precise incar \n",
    "* single point : band structure, fukui, PARCHARG, ...\n",
    "* relaxation : high/ low precision\n",
    "* custom : you have to define ot in the code \n",
    "\n",
    "create another folder with same architecture with \"rerun\" jobs \n",
    "\n",
    "=> __ADD YOUR OWN__  ! "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-9107ff0cacfc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-9107ff0cacfc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    READ => pytohn3 read_job.py\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "READ => pytohn3 read_job.py  \n",
    "* read jobs & report their convergence status (pre-run / unconverged / still running / converged)\n",
    "* filter the runs (all, converged, minimal at each x or on hull) + one-by-one selection\n",
    "* do an analysis for this restricted list (BADER, DOS, COOP, DISTANCE HISTOGRAM, ... ) => __ADD YOUR OWN__  !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ") as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n\\n\\n\".join([ str(c_e) for c_e in tmp_list])\n397/40:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n\\n\\n\".join([ str(c_e) for c_e in tmp_list]))\n397/41:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsng === \\n\")\nprint(\"\\n\\n\\n\".join([ str(c_e) for c_e in tmp_list]))\n397/42:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n\".join([ str(c_e) for c_e in tmp_list]))\n397/43:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n NEW ENTRY\".join([ str(c_e) for c_e in tmp_list]))\n397/44:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n --> NEW ENTRY \\n\".join([ str(c_e) for c_e in tmp_list]))\n397/45:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n --> NEW ENTRY \\n\".join([ \"--> NEW ENTRY\", str(c_e) for c_e in tmp_list]))\n397/46:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n --> NEW ENTRY \\n\".join([ \"--> NEW ENTRY\"+str(c_e) for c_e in tmp_list]))\n397/47:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n --> NEW ENTRY \\n\".join([ \"--> NEW ENTRY\\n\"+str(c_e) for c_e in tmp_list]))\n397/48:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n\".join([ \"--> NEW ENTRY\\n\"+str(c_e) for c_e in tmp_list]))\n397/49:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n\".join([ \"--> ENTRY nb {}\\n\".format(i)+str(c_e) for i,c_e in enumerate(tmp_list)]))\n397/50:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n\".join([ \"--> ENTRY nb {}\\n\".format(i+1)+str(c_e) for i,c_e in enumerate(tmp_list)]))\n397/51:\nfrom multiprocessing import Pool, cpu_count\nimport time\nimport os\n\nlist_of_folder_to_parse = [\"H2O2_no_vasprun\", \"H2O2_converged_run\"] \n\npersonalized_drone = VaspToComputedEntryDrone(inc_structure=True,\n                                            parameters=[\"incar\"],\n                                            data=['efermi', \"converged\"])\nsimple_drone = SimpleVaspToComputedEntryDrone(inc_structure=True)\n\ndef parse_vasprun(folder):\n    \"a very loooong function\"\n    print(\"in folder\", folder)\n    computed_entry = personalized_drone.assimilate(folder) \n    if computed_entry is None :\n        computed_entry = simple_drone.assimilate(folder) \n      \n    return(computed_entry)\n\nwith Pool(processes=cpu_count()) as parallel_runs: # with statement = context manager\n        tmp_list = parallel_runs.map(parse_vasprun , list_of_folder_to_parse ) \n        parallel_runs.close() # close the threads after execution\n        parallel_runs.join()  # wait for the slow ones\nprint(\"\\n === finished parsing === \\n\")\n\nprint(\"\\n\\n\\n\".join([ \"--> ENTRY {}\\n\".format(i+1)+str(c_e) for i,c_e in enumerate(tmp_list)]))\n400/1: write\n400/4: rerun\n400/5: rerun\n400/6: rerun\n400/7: rerun.\n400/8: write_job\n400/9:\n# read_write  : vasp workflow at the ICGM\nworkflow\n400/10:\n# read_write  : vasp workflow at the ICGM\nworkflow\n400/11:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif (ordering, distortion, slabs, ...) (maybe get multiple structures)\n* generate vasp input (POTCAR, INCAR) \n* launch jobs\n\n__RERUN LOOP__\n* check convergence \n* modify incar (create \"reruns\")\n* launch the \"reruns\"\n\n__ANALYZE LOOP__\n* anayze converged job \n* launch analysis requiring vasp job (phonopy, lobster, bansstructure, ... )\n* rerun loop on the analysez folders\n400/12:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif (ordering, distortion, slabs, ...) (maybe get multiple structures)\n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* check convergence \n* modify incar (create \"reruns\")\n* `launch \"rerun\" jobs`\n\n__ANALYZE LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, bansstructure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for analysis jobs\n400/13:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif (ordering, distortion, slabs, ...) (maybe get multiple structures)\n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* check convergence \n* modify incar (create \"reruns\")\n* `launch \"rerun\" jobs`\n\n__READ LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, bansstructure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for analysis jobs\n400/14:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif (ordering, distortion, slabs, ...) (maybe get multiple structures)\n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* check convergence \n* modify incar (create \"reruns\")\n* `launch \"rerun\" jobs`\n\n__READ LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, bansstructure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for analysis jobs\n400/16:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif (ordering, distortion, slabs, ...) (maybe get multiple structures)\n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* check convergence \n* modify incar (create \"reruns\")\n* `launch \"rerun\" jobs`\n\n__READ LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, band structure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for \"analysis\" jobs\n400/17:\n\nWRITE LOOP => python3 ./write_job \n\n* get cif from internet / experimentalists\n* modify cif \n * (ordering, distortion, slabs, ...)\n * lol\n* generate vasp input (POTCAR, INCAR)\n400/18:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif \n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* check convergence \n* modify incar (create \"reruns\")\n* `launch \"rerun\" jobs`\n\n__READ LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, band structure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for \"analysis\" jobs\n400/19:\n\nWRITE LOOP => python3 ./write_job \n\n* get cif from internet / experimentalists\n* modify cif (ordering, desodiation, distortion, layer rotation, slabs, => add your own ! \n* generate vasp input (POTCAR, INCAR)\n400/20:\nLAUNCH => python3 ./launch_job.py\n\nask for sbatch parameters (name, nb core / job, QOS, ...)\nask for job executable parameters (single, double,...)\narray or individual launch \nif array : launch 1 job / subfolder all at once \nelse : ask for each subfolder if user want to launch is as a job\n400/21:\nLAUNCH => python3 ./launch_job.py\n\nask for sbatch parameters (name, nb core / job, QOS, ...)\n\nask for job executable parameters (single, double,...)\n\nask array or individual launch \n* if array : launch 1 job / subfolder all at once \n* else : ask for each subfolder if user want to launch is as a job\n400/22:\n\nWRITE LOOP => python3 ./write_job.py\n\n* get cif from internet / experimentalists\n* modify cif (ordering, desodiation, distortion, layer rotation, slabs, => __ADD YOUR OWN__  ! \n* generate vasp input (POTCAR, INCAR)\n400/23:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif \n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* read existing job (converged or not)\n* modify incar (create \"reruns\")\n* `launch \"rerun\" jobs`\n\n__READ LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, band structure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for \"analysis\" jobs\n400/24:\n# read_write  : vasp workflow at the ICGM\n## vasp workflow : \n__WRITE LOOP__\n* get cif from internet / experimentalists \n* modify cif \n* generate vasp input (POTCAR, INCAR) \n* `launch jobs`\n\n__RERUN LOOP__\n* read existing job (converged or not)\n* modify incar (create \"rerun\" vasp input file)\n* `launch \"rerun\" jobs`\n\n__READ LOOP__\n* anayze converged job \n* write \"analysis\" vasp job (phonopy, lobster, band structure, ... )\n* `launch \"analysis\" jobs`\n* __RERUN LOOP__ for \"analysis\" jobs\n400/25:\nRERUN => python3 ./rerun_job.py\n\nask for rerun type (single point, relaxation, custom ...) \n\nask for more precise incar \n* single point : band structure, fukui, PARCHARG, ...\n* relaxation : high/ low precision\n* custom : you have to define ot in the code \n\ncreate another folder with same architecture with \"rerun\" jobs \n\n=> __ADD YOUR OWN__  !\n400/26:\nREAD => pytohn3 read_job.py  \nread jobs\nreport their convergence status (pre-run / unconverged / still running / converged)\n400/27:\nREAD => pytohn3 read_job.py  \n* read jobs & report their convergence status (pre-run / unconverged / still running / converged)\n* filter the runs (all, converged, minimal at each x or on hull) + one-by-one selection\n* do an analysis for this restricted list (BADER, DOS, COOP, DISTANCE HISTOGRAM, ... ) => __ADD YOUR OWN__  !\n402/1:\na = 1  \nprint(a,'type', type(a))\na = 1.00005 \nprint(a,'type', type(a))\na = \"hello world\" \nprint(a,'type', type(a))\n402/2:\na = 2+3\nprint(a,'type', type(a))\na = 1.00005 \nprint(a,'type', type(a))\na = \"hello world\" \nprint(a,'type', type(a))\n   1: %history -ge\n"
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}